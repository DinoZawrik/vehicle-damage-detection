{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vehicle Damage Detection - Advanced Usage\n",
    "\n",
    "This notebook demonstrates advanced features of the Vehicle Damage Detection System including:\n",
    "- Batch processing\n",
    "- Async tasks with Celery\n",
    "- Custom model configuration\n",
    "- Performance monitoring\n",
    "- Error handling strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import concurrent.futures\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "API_URL = \"http://localhost:8000\"\n",
    "\n",
    "print(\"Advanced Vehicle Damage Detection Demo\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Performance Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_analysis(image_paths, description=\"Benchmark\"):\n",
    "    \"\"\"\n",
    "    Benchmark analysis performance for multiple images.\n",
    "    \n",
    "    Args:\n",
    "        image_paths: List of image file paths\n",
    "        description: Description for the benchmark\n",
    "    \n",
    "    Returns:\n",
    "        Performance metrics dictionary\n",
    "    \"\"\"\n",
    "    print(f\"\\nüöÄ Starting {description}\")\n",
    "    print(f\"Images to process: {len(image_paths)}\")\n",
    "    \n",
    "    results = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i, image_path in enumerate(image_paths, 1):\n",
    "        print(f\"Processing {i}/{len(image_paths)}: {Path(image_path).name}\")\n",
    "        \n",
    "        try:\n",
    "            img_start = time.time()\n",
    "            \n",
    "            with open(image_path, \"rb\") as f:\n",
    "                files = {\"file\": (image_path, f, \"image/jpeg\")}\n",
    "                response = requests.post(f\"{API_URL}/api/analyze\", files=files, timeout=30)\n",
    "            \n",
    "            img_time = time.time() - img_start\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                results.append({\n",
    "                    'image': Path(image_path).name,\n",
    "                    'status': 'success',\n",
    "                    'processing_time': img_time,\n",
    "                    'detections': result.get('detection', {}).get('num_detections', 0),\n",
    "                    'severity': result.get('classification', {}).get('severity'),\n",
    "                    'estimated_cost': result.get('cost_estimate', {}).get('estimated_cost', 0)\n",
    "                })\n",
    "                print(f\"   ‚úÖ Completed in {img_time:.2f}s\")\n",
    "            else:\n",
    "                results.append({\n",
    "                    'image': Path(image_path).name,\n",
    "                    'status': 'failed',\n",
    "                    'processing_time': img_time,\n",
    "                    'error': response.status_code\n",
    "                })\n",
    "                print(f\"   ‚ùå Failed: {response.status_code}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            results.append({\n",
    "                'image': Path(image_path).name,\n",
    "                'status': 'error',\n",
    "                'processing_time': 0,\n",
    "                'error': str(e)\n",
    "            })\n",
    "            print(f\"   ‚ùå Error: {str(e)[:50]}...\")\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    # Calculate metrics\n",
    "    successful = [r for r in results if r['status'] == 'success']\n",
    "    \n",
    "    metrics = {\n",
    "        'total_images': len(image_paths),\n",
    "        'successful': len(successful),\n",
    "        'failed': len(results) - len(successful),\n",
    "        'success_rate': len(successful) / len(image_paths) * 100,\n",
    "        'total_time': total_time,\n",
    "        'avg_time_per_image': total_time / len(image_paths),\n",
    "        'images_per_second': len(successful) / total_time if total_time > 0 else 0\n",
    "    }\n",
    "    \n",
    "    if successful:\n",
    "        processing_times = [r['processing_time'] for r in successful]\n",
    "        metrics.update({\n",
    "            'min_time': min(processing_times),\n",
    "            'max_time': max(processing_times),\n",
    "            'median_time': sorted(processing_times)[len(processing_times)//2]\n",
    "        })\n",
    "    \n",
    "    print(f\"\\nüìä BENCHMARK RESULTS:\")\n",
    "    print(f\"   ‚Ä¢ Total time: {total_time:.2f}s\")\n",
    "    print(f\"   ‚Ä¢ Success rate: {metrics['success_rate']:.1f}%\")\n",
    "    print(f\"   ‚Ä¢ Avg time per image: {metrics['avg_time_per_image']:.2f}s\")\n",
    "    print(f\"   ‚Ä¢ Throughput: {metrics['images_per_second']:.2f} images/sec\")\n",
    "    \n",
    "    return pd.DataFrame(results), metrics\n",
    "\n",
    "# Example usage (uncomment and replace with your image paths)\n",
    "# image_paths = [\n",
    "#     \"path/to/image1.jpg\",\n",
    "#     \"path/to/image2.jpg\",\n",
    "#     \"path/to/image3.jpg\"\n",
    "# ]\n",
    "# results_df, metrics = benchmark_analysis(image_paths, \"Test Benchmark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Async Task Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_async_task(image_path, task_type=\"single\"):\n",
    "    \"\"\"\n",
    "    Submit an async task for processing.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to image or list of paths\n",
    "        task_type: \"single\" or \"batch\"\n",
    "    \n",
    "    Returns:\n",
    "        Task ID or response\n",
    "    \"\"\"\n",
    "    # Note: This assumes you have Celery endpoints implemented\n",
    "    # For now, we'll simulate with direct API calls\n",
    "    \n",
    "    if task_type == \"batch\":\n",
    "        # This would be a batch processing endpoint\n",
    "        print(\"Submitting batch task...\")\n",
    "        # response = requests.post(f\"{API_URL}/api/batch-analyze\", json={\"image_paths\": image_path})\n",
    "    else:\n",
    "        # Single async task\n",
    "        print(\"Submitting single async task...\")\n",
    "        # response = requests.post(f\"{API_URL}/api/analyze-async\", json={\"image_path\": image_path})\n",
    "    \n",
    "    # For now, return mock task ID\n",
    "    return {\"task_id\": f\"mock_task_{int(time.time())}\", \"status\": \"submitted\"}\n",
    "\n",
    "def monitor_task(task_id, max_wait=300):\n",
    "    \"\"\"\n",
    "    Monitor task progress.\n",
    "    \n",
    "    Args:\n",
    "        task_id: Task ID to monitor\n",
    "        max_wait: Maximum wait time in seconds\n",
    "    \n",
    "    Returns:\n",
    "        Final task result\n",
    "    \"\"\"\n",
    "    print(f\"Monitoring task: {task_id}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    while time.time() - start_time < max_wait:\n",
    "        # This would check task status via Celery\n",
    "        # response = requests.get(f\"{API_URL}/api/task-status/{task_id}\")\n",
    "        \n",
    "        # For now, simulate task completion\n",
    "        time.sleep(2)\n",
    "        elapsed = time.time() - start_time\n",
    "        print(f\"   Task still running... ({elapsed:.0f}s elapsed)\")\n",
    "        \n",
    "        # Simulate completion after 10 seconds\n",
    "        if elapsed > 10:\n",
    "            print(\"‚úÖ Task completed!\")\n",
    "            return {\"task_id\": task_id, \"status\": \"completed\", \"result\": \"mock_result\"}\n",
    "    \n",
    "    print(\"‚è∞ Task timeout\")\n",
    "    return {\"task_id\": task_id, \"status\": \"timeout\"}\n",
    "\n",
    "# Example async workflow\n",
    "# task = submit_async_task(\"path/to/image.jpg\", \"single\")\n",
    "# result = monitor_task(task[\"task_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Error Analysis and Retry Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_with_retry(image_path, max_retries=3, backoff_factor=2):\n",
    "    \"\"\"\n",
    "    Analyze image with retry logic for robust processing.\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to image\n",
    "        max_retries: Maximum number of retries\n",
    "        backoff_factor: Backoff factor for exponential backoff\n",
    "    \n",
    "    Returns:\n",
    "        Analysis result or None if all retries failed\n",
    "    \"\"\"\n",
    "    for attempt in range(max_retries + 1):\n",
    "        try:\n",
    "            print(f\"Attempt {attempt + 1}/{max_retries + 1}: {Path(image_path).name}\")\n",
    "            \n",
    "            with open(image_path, \"rb\") as f:\n",
    "                files = {\"file\": (image_path, f, \"image/jpeg\")}\n",
    "                response = requests.post(f\"{API_URL}/api/analyze\", files=files, timeout=60)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                print(f\"   ‚úÖ Success on attempt {attempt + 1}\")\n",
    "                return response.json()\n",
    "            elif response.status_code == 503:\n",
    "                print(f\"   ‚ö†Ô∏è  Service unavailable, retrying in {2**attempt}s...\")\n",
    "                time.sleep(2 ** attempt)\n",
    "            elif response.status_code == 429:\n",
    "                print(f\"   ‚ö†Ô∏è  Rate limited, retrying in {5*backoff_factor**attempt}s...\")\n",
    "                time.sleep(5 * backoff_factor ** attempt)\n",
    "            else:\n",
    "                print(f\"   ‚ùå HTTP {response.status_code}: {response.text[:100]}\")\n",
    "                if attempt == max_retries:\n",
    "                    return None\n",
    "                time.sleep(1)\n",
    "                \n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"   ‚è∞ Timeout, retrying in {3**attempt}s...\")\n",
    "            time.sleep(3 ** attempt)\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            print(f\"   üîå Connection error, retrying in {5**attempt}s...\")\n",
    "            time.sleep(5 ** attempt)\n",
    "        except Exception as e:\n",
    "            print(f\"   üí• Unexpected error: {str(e)[:50]}...\")\n",
    "            if attempt == max_retries:\n",
    "                return None\n",
    "            time.sleep(1)\n",
    "    \n",
    "    print(f\"   ‚ùå All {max_retries + 1} attempts failed\")\n",
    "    return None\n",
    "\n",
    "def batch_analyze_with_retry(image_paths, max_workers=3):\n",
    "    \"\"\"\n",
    "    Batch analyze with retry logic and concurrency.\n",
    "    \n",
    "    Args:\n",
    "        image_paths: List of image paths\n",
    "        max_workers: Maximum number of concurrent workers\n",
    "    \n",
    "    Returns:\n",
    "        Results dataframe\n",
    "    \"\"\"\n",
    "    print(f\"üöÄ Batch processing {len(image_paths)} images with retry logic\")\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit all tasks\n",
    "        future_to_path = {\n",
    "            executor.submit(analyze_with_retry, path): path \n",
    "            for path in image_paths\n",
    "        }\n",
    "        \n",
    "        # Process results as they complete\n",
    "        for future in concurrent.futures.as_completed(future_to_path):\n",
    "            path = future_to_path[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                \n",
    "                if result:\n",
    "                    # Extract key metrics\n",
    "                    classification = result.get('classification', {})\n",
    "                    cost_estimate = result.get('cost_estimate', {})\n",
    "                    \n",
    "                    results.append({\n",
    "                        'image': Path(path).name,\n",
    "                        'status': 'success',\n",
    "                        'damage_count': classification.get('damage_count', 0),\n",
    "                        'severity': classification.get('severity'),\n",
    "                        'estimated_cost': cost_estimate.get('estimated_cost', 0),\n",
    "                        'processing_time': result.get('total_processing_time', 0)\n",
    "                    })\n",
    "                else:\n",
    "                    results.append({\n",
    "                        'image': Path(path).name,\n",
    "                        'status': 'failed',\n",
    "                        'damage_count': 0,\n",
    "                        'severity': None,\n",
    "                        'estimated_cost': 0,\n",
    "                        'processing_time': 0\n",
    "                    })\n",
    "                    \n",
    "            except Exception as e:\n",
    "                results.append({\n",
    "                    'image': Path(path).name,\n",
    "                    'status': 'error',\n",
    "                    'damage_count': 0,\n",
    "                    'severity': None,\n",
    "                    'estimated_cost': 0,\n",
    "                    'processing_time': 0,\n",
    "                    'error': str(e)\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Example usage\n",
    "# results_df = batch_analyze_with_retry(image_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Performance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(results_df):\n",
    "    \"\"\"\n",
    "    Create visualizations from analysis results.\n",
    "    \n",
    "    Args:\n",
    "        results_df: DataFrame with analysis results\n",
    "    \"\"\"\n",
    "    if results_df.empty or len(results_df[results_df['status'] == 'success']) == 0:\n",
    "        print(\"No successful results to visualize\")\n",
    "        return\n",
    "    \n",
    "    # Filter successful results\n",
    "    successful = results_df[results_df['status'] == 'success'].copy()\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('Vehicle Damage Analysis Results', fontsize=16)\n",
    "    \n",
    "    # 1. Processing time distribution\n",
    "    axes[0, 0].hist(successful['processing_time'], bins=10, alpha=0.7, color='skyblue')\n",
    "    axes[0, 0].set_title('Processing Time Distribution')\n",
    "    axes[0, 0].set_xlabel('Time (seconds)')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    \n",
    "    # 2. Damage count distribution\n",
    "    damage_counts = successful['damage_count'].value_counts().sort_index()\n",
    "    axes[0, 1].bar(damage_counts.index, damage_counts.values, color='lightcoral')\n",
    "    axes[0, 1].set_title('Damage Count Distribution')\n",
    "    axes[0, 1].set_xlabel('Number of Damages')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    \n",
    "    # 3. Severity distribution\n",
    "    severity_counts = successful['severity'].value_counts()\n",
    "    colors = {'minor': 'green', 'moderate': 'yellow', 'severe': 'orange', 'critical': 'red'}\n",
    "    severity_colors = [colors.get(sev, 'gray') for sev in severity_counts.index]\n",
    "    axes[1, 0].pie(severity_counts.values, labels=severity_counts.index, \n",
    "                   colors=severity_colors, autopct='%1.1f%%')\n",
    "    axes[1, 0].set_title('Damage Severity Distribution')\n",
    "    \n",
    "    # 4. Cost distribution\n",
    "    costs = successful[successful['estimated_cost'] > 0]['estimated_cost']\n",
    "    if len(costs) > 0:\n",
    "        axes[1, 1].hist(costs, bins=10, alpha=0.7, color='gold')\n",
    "        axes[1, 1].set_title('Cost Distribution')\n",
    "        axes[1, 1].set_xlabel('Cost (USD)')\n",
    "        axes[1, 1].set_ylabel('Frequency')\n",
    "    else:\n",
    "        axes[1, 1].text(0.5, 0.5, 'No cost data available', \n",
    "                        ha='center', va='center', transform=axes[1, 1].transAxes)\n",
    "        axes[1, 1].set_title('Cost Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\nüìä SUMMARY STATISTICS:\")\n",
    "    print(f\"   ‚Ä¢ Total images processed: {len(results_df)}\")\n",
    "    print(f\"   ‚Ä¢ Success rate: {(len(successful) / len(results_df) * 100):.1f}%\")\n",
    "    print(f\"   ‚Ä¢ Avg processing time: {successful['processing_time'].mean():.2f}s\")\n",
    "    print(f\"   ‚Ä¢ Avg damage count: {successful['damage_count'].mean():.1f}\")\n",
    "    \n",
    "    if len(costs) > 0:\n",
    "        print(f\"   ‚Ä¢ Avg estimated cost: ${costs.mean():.2f}\")\n",
    "        print(f\"   ‚Ä¢ Max estimated cost: ${costs.max():.2f}\")\n",
    "    \n",
    "    # Success rate by status\n",
    "    status_counts = results_df['status'].value_counts()\n",
    "    print(f\"\\nüìã PROCESSING STATUS:\")\n",
    "    for status, count in status_counts.items():\n",
    "        print(f\"   ‚Ä¢ {status}: {count}\")\n",
    "\n",
    "# Example usage\n",
    "# if 'results_df' in locals():\n",
    "#     visualize_results(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_results(results_df, filename=None, format='excel'):\n",
    "    \"\"\"\n",
    "    Export analysis results to various formats.\n",
    "    \n",
    "    Args:\n",
    "        results_df: DataFrame with results\n",
    "        filename: Output filename (optional)\n",
    "        format: 'excel', 'csv', or 'json'\n",
    "    \"\"\"\n",
    "    if filename is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"vehicle_damage_analysis_{timestamp}\"\n",
    "    \n",
    "    if format.lower() == 'excel':\n",
    "        output_file = f\"{filename}.xlsx\"\n",
    "        with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "            # Main results\n",
    "            results_df.to_excel(writer, sheet_name='Results', index=False)\n",
    "            \n",
    "            # Summary statistics\n",
    "            summary = pd.DataFrame({\n",
    "                'Metric': ['Total Images', 'Success Rate (%)', 'Avg Processing Time (s)', \n",
    "                          'Avg Damage Count', 'Avg Cost (USD)'],\n",
    "                'Value': [\n",
    "                    len(results_df),\n",
    "                    (results_df['status'] == 'success').mean() * 100,\n",
    "                    results_df[results_df['status'] == 'success']['processing_time'].mean(),\n",
    "                    results_df[results_df['status'] == 'success']['damage_count'].mean(),\n",
    "                    results_df[results_df['status'] == 'success']['estimated_cost'].mean()\n",
    "                ]\n",
    "            })\n",
    "            summary.to_excel(writer, sheet_name='Summary', index=False)\n",
    "        \n",
    "    elif format.lower() == 'csv':\n",
    "        output_file = f\"{filename}.csv\"\n",
    "        results_df.to_csv(output_file, index=False)\n",
    "        \n",
    "    elif format.lower() == 'json':\n",
    "        output_file = f\"{filename}.json\"\n",
    "        results_df.to_json(output_file, orient='records', indent=2)\n",
    "    \n",
    "    print(f\"üìÅ Results exported to: {output_file}\")\n",
    "    return output_file\n",
    "\n",
    "# Example usage\n",
    "# export_file = export_results(results_df, format='excel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Complete Advanced Workflow\n",
    "\n",
    "Putting it all together - a comprehensive analysis workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_analysis(image_paths, max_workers=3, export=True):\n",
    "    \"\"\"\n",
    "    Complete advanced analysis workflow.\n",
    "    \n",
    "    Args:\n",
    "        image_paths: List of image paths\n",
    "        max_workers: Number of concurrent workers\n",
    "        export: Whether to export results\n",
    "    \n",
    "    Returns:\n",
    "        Analysis results DataFrame\n",
    "    \"\"\"\n",
    "    print(\"üéØ COMPREHENSIVE VEHICLE DAMAGE ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Step 1: Health check\n",
    "    print(\"\\n1. üîç System Health Check...\")\n",
    "    try:\n",
    "        response = requests.get(f\"{API_URL}/health\", timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            health = response.json()\n",
    "            print(f\"   ‚úÖ System: {health.get('status')}\")\n",
    "            print(f\"   ‚úÖ Model: {'Ready' if health.get('model_loaded') else 'Not loaded'}\")\n",
    "        else:\n",
    "            print(\"   ‚ùå System health check failed\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Cannot connect to API: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Step 2: Performance benchmark\n",
    "    print(f\"\\n2. üìä Performance Benchmark...\")\n",
    "    benchmark_results, metrics = benchmark_analysis(image_paths[:5], \"Quick Benchmark\")\n",
    "    \n",
    "    # Step 3: Full analysis with retry logic\n",
    "    print(f\"\\n3. üîÑ Full Analysis with Retry Logic...\")\n",
    "    full_results = batch_analyze_with_retry(image_paths, max_workers=max_workers)\n",
    "    \n",
    "    # Step 4: Results visualization\n",
    "    print(f\"\\n4. üìà Results Visualization...\")\n",
    "    visualize_results(full_results)\n",
    "    \n",
    "    # Step 5: Export results\n",
    "    if export:\n",
    "        print(f\"\\n5. üíæ Exporting Results...\")\n",
    "        export_file = export_results(full_results, format='excel')\n",
    "    \n",
    "    print(f\"\\n‚úÖ ANALYSIS COMPLETE!\")\n",
    "    print(f\"   ‚Ä¢ Processed: {len(full_results)} images\")\n",
    "    print(f\"   ‚Ä¢ Success rate: {(full_results['status'] == 'success').mean()*100:.1f}%\")\n",
    "    print(f\"   ‚Ä¢ Average cost: ${full_results[full_results['status'] == 'success']['estimated_cost'].mean():.2f}\")\n",
    "    \n",
    "    return full_results\n",
    "\n",
    "# Example usage - replace with your image paths\n",
    "# image_paths = [\n",
    "#     \"path/to/car1.jpg\",\n",
    "#     \"path/to/car2.jpg\", \n",
    "#     \"path/to/car3.jpg\"\n",
    "# ]\n",
    "# results = comprehensive_analysis(image_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Best Practices Summary\n",
    "\n",
    "### Performance Optimization\n",
    "1. **Concurrent Processing**: Use ThreadPoolExecutor for parallel API calls\n",
    "2. **Retry Logic**: Implement exponential backoff for transient failures\n",
    "3. **Timeout Management**: Set appropriate timeouts based on image size\n",
    "4. **Batch Processing**: Use async tasks for large workloads\n",
    "\n",
    "### Error Handling\n",
    "1. **Network Issues**: Implement connection retry logic\n",
    "2. **Service Unavailability**: Handle 503 responses gracefully\n",
    "3. **Rate Limiting**: Respect rate limits with backoff\n",
    "4. **Validation**: Check image format and size before upload\n",
    "\n",
    "### Monitoring & Observability\n",
    "1. **Performance Metrics**: Track processing time and success rates\n",
    "2. **Error Tracking**: Log failures with detailed error information\n",
    "3. **Resource Usage**: Monitor memory and CPU usage\n",
    "4. **Health Checks**: Regular system health verification\n",
    "\n",
    "### Data Management\n",
    "1. **Result Storage**: Export results to Excel/CSV for analysis\n",
    "2. **Historical Data**: Track analysis history for trends\n",
    "3. **Quality Control**: Review failed analyses for patterns\n",
    "4. **Backup**: Regular backup of analysis results\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. **Model Fine-tuning**: Train custom models on your specific dataset\n",
    "2. **API Enhancement**: Add custom endpoints for specific use cases\n",
    "3. **Real-time Processing**: Implement WebSocket for real-time updates\n",
    "4. **Integration**: Connect with insurance systems or databases\n",
    "5. **Mobile App**: Develop mobile application for field inspections"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}