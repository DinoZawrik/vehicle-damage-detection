"""
–£–ª—É—á—à–µ–Ω–Ω—ã–π YOLOv9n Vehicle Damage Detection API —Å –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π —Å–∏—Å—Ç–µ–º–æ–π.

–ö–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç YOLO, SAM, CLIP –∏ LLM –¥–ª—è SOTA –∞–Ω–∞–ª–∏–∑–∞ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π.
"""

from fastapi import FastAPI, File, UploadFile, HTTPException, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from PIL import Image
import uvicorn
import sqlite3
import os
import shutil
from datetime import datetime
from typing import List, Dict, Any
import json
import logging
import torch
import cv2
import numpy as np
import asyncio

# –ò–º–ø–æ—Ä—Ç –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ pipeline
from src.models.multi_modal_pipeline import MultiModalPipeline

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –°–æ–∑–¥–∞–µ–º FastAPI –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
app = FastAPI(
    title="Multi-Modal Vehicle Damage Detection API",
    description="SOTA —Å–∏—Å—Ç–µ–º–∞ –∞–Ω–∞–ª–∏–∑–∞ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–π —Å YOLO + SAM + CLIP + LLM",
    version="4.0.0-multi-modal"
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
UPLOAD_DIR = "./uploads"
MAX_FILE_SIZE = 10 * 1024 * 1024  # 10MB
DB_PATH = "./data/detection.db"
CONFIDENCE_THRESHOLD = 0.35
MODEL_PATH = "yolov8n.pt"

# –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
os.makedirs(UPLOAD_DIR, exist_ok=True)
os.makedirs("./data", exist_ok=True)

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
pipeline = None

def init_db():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è SQLite –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS detections (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            filename TEXT NOT NULL,
            filepath TEXT NOT NULL,
            timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
            result TEXT,
            confidence REAL,
            processing_time REAL,
            model_version TEXT,
            modalities TEXT
        )
    ''')
    
    conn.commit()
    conn.close()

def init_pipeline():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ pipeline"""
    global pipeline
    try:
        logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–≥–æ pipeline...")
        
        pipeline = MultiModalPipeline(
            yolo_model_path=MODEL_PATH,
            yolo_conf_threshold=CONFIDENCE_THRESHOLD,
            enable_sam=True,
            enable_clip=True,
            enable_llm=True,
            device="auto"
        )
        
        logger.info("‚úÖ –ú—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω—ã–π pipeline —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω!")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ pipeline: {e}")
        return False

@app.on_event("startup")
async def startup_event():
    """–°–æ–±—ã—Ç–∏–µ –∑–∞–ø—É—Å–∫–∞ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
    global pipeline

    init_db()

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º pipeline
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã –∞–Ω–∞–ª–∏–∑–∞ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π...")
    if not init_pipeline():
        logger.warning("‚ö†Ô∏è Pipeline –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω - –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –º–æ–∂–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ")

@app.get("/")
async def root():
    """–ö–æ—Ä–Ω–µ–≤–æ–π —ç–Ω–¥–ø–æ–∏–Ω—Ç"""
    pipeline_status = "loaded" if pipeline is not None else "loading"
    
    return {
        "service": "Multi-Modal Vehicle Damage Detection API",
        "version": "4.0.0-multi-modal",
        "status": "running",
        "pipeline_status": pipeline_status,
        "features": [
            "yolo_detection",
            "sam_segmentation",
            "clip_semantic_analysis",
            "llm_reporting",
            "ensemble_predictions",
            "precise_area_calculation",
            "professional_reports"
        ],
        "model_config": {
            "confidence_threshold": CONFIDENCE_THRESHOLD,
            "model_size": "nano",
            "modalities": ["yolo", "sam", "clip", "llm"]
        },
        "docs": "/docs",
        "health": "/health"
    }

@app.get("/health")
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–∏—Å—Ç–µ–º—ã"""
    pipeline_status = "ready" if pipeline is not None else "loading"
    
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "service": "multi-modal-vehicle-damage-detection",
        "version": "4.0.0-multi-modal",
        "pipeline_status": pipeline_status,
        "torch_version": torch.__version__ if torch else "not_available",
        "cuda_available": torch.cuda.is_available() if torch else False,
        "modalities_available": {
            "yolo": pipeline is not None,
            "sam": pipeline is not None and hasattr(pipeline, 'sam_segmentor') and pipeline.sam_segmentor is not None,
            "clip": pipeline is not None and hasattr(pipeline, 'clip_analyzer') and pipeline.clip_analyzer is not None,
            "llm": pipeline is not None and hasattr(pipeline, 'llm_analyzer') and pipeline.llm_analyzer is not None
        }
    }

async def process_image(
    file: UploadFile,
    include_sam: bool = True,
    include_clip: bool = True,
    include_llm: bool = True,
    visualize: bool = True
) -> Dict[str, Any]:
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–º–∏ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è–º–∏"""
    
    global pipeline
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–∞
    file.file.seek(0, os.SEEK_END)
    file_size = file.file.tell()
    file.file.seek(0)

    if file_size > MAX_FILE_SIZE:
        raise HTTPException(status_code=413, detail=f"–§–∞–π–ª —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π. –ú–∞–∫—Å–∏–º—É–º {MAX_FILE_SIZE // (1024*1024)}MB")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∞ —Ñ–∞–π–ª–∞
    if not file.content_type or not file.content_type.startswith('image/'):
        raise HTTPException(status_code=400, detail="–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è —Ç–æ–ª—å–∫–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")

    # –ì–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ pipeline
    if pipeline is None:
        logger.warning("‚ö†Ô∏è Pipeline –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω! –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é —Å–µ–π—á–∞—Å...")
        if not init_pipeline():
            raise HTTPException(status_code=500, detail="–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ pipeline")
    
    try:
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{timestamp}_{file.filename}"
        filepath = os.path.join(UPLOAD_DIR, filename)

        with open(filepath, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)

        # –í—ã–ø–æ–ª–Ω—è–µ–º –∞–Ω–∞–ª–∏–∑
        logger.info(f"–ê–Ω–∞–ª–∏–∑ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π –≤ —Ñ–∞–π–ª–µ {filename}...")
        start_time = datetime.now()

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        img = cv2.imread(filepath)
        if img is None:
            raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ")

        # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π pipeline —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π
        temp_pipeline = MultiModalPipeline(
            yolo_model_path=MODEL_PATH,
            yolo_conf_threshold=CONFIDENCE_THRESHOLD,
            enable_sam=include_sam,
            enable_clip=include_clip,
            enable_llm=include_llm,
            device="auto"
        )

        # –í—ã–ø–æ–ª–Ω—è–µ–º –∞–Ω–∞–ª–∏–∑
        analysis_results = temp_pipeline.analyze_image(img, visualize=visualize)

        end_time = datetime.now()
        processing_time = (end_time - start_time).total_seconds()
        
        # –°–æ–±–∏—Ä–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è—Ö
        modalities = []
        if include_sam:
            modalities.append("sam")
        if include_clip:
            modalities.append("clip")
        if include_llm:
            modalities.append("llm")
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        result = {
            "analysis_results": analysis_results,
            "timestamp": timestamp,
            "processing_time": processing_time,
            "modalities": modalities,
            "config": {
                "include_sam": include_sam,
                "include_clip": include_clip,
                "include_llm": include_llm,
                "visualize": visualize
            }
        }
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –ë–î
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        
        # –ü–æ–ª—É—á–∞–µ–º –æ—Å–Ω–æ–≤–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –±–∞–∑—ã
        yolo_detections = analysis_results.get("yolo", {}).get("detections", [])
        avg_confidence = np.mean([d.get("confidence", 0.0) for d in yolo_detections]) if yolo_detections else 0.0
        
        cursor.execute('''
            INSERT INTO detections (filename, filepath, result, confidence, processing_time, model_version, modalities)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        ''', (
            filename,
            filepath,
            json.dumps(result),
            avg_confidence,
            processing_time,
            f"Multi-Modal v4.0 (SAM: {include_sam}, CLIP: {include_clip}, LLM: {include_llm})",
            json.dumps(modalities)
        ))
        
        conn.commit()
        conn.close()
        
        return result
        
    except Exception as e:
        logger.error(f"Error processing image: {e}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {str(e)}")

@app.post("/detect")
async def detect_damage(
    file: UploadFile = File(...),
    include_sam: bool = Query(True, description="–í–∫–ª—é—á–∏—Ç—å SAM —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—é"),
    include_clip: bool = Query(True, description="–í–∫–ª—é—á–∏—Ç—å CLIP –∞–Ω–∞–ª–∏–∑"),
    include_llm: bool = Query(True, description="–í–∫–ª—é—á–∏—Ç—å LLM –∞–Ω–∞–ª–∏–∑"),
    visualize: bool = Query(True, description="–ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é")
):
    """–£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π endpoint –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–∏–π —Å –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–º–∏ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è–º–∏"""
    
    return await process_image(file, include_sam, include_clip, include_llm, visualize)

@app.post("/detect/basic")
async def detect_damage_basic(file: UploadFile = File(...)):
    """–ë–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–æ–ª—å–∫–æ —Å YOLO (–±—ã—Å—Ç—Ä—ã–π)"""
    return await process_image(file, include_sam=False, include_clip=False, include_llm=False, visualize=True)

@app.post("/detect/segmented")
async def detect_damage_segmented(file: UploadFile = File(...)):
    """–ê–Ω–∞–ª–∏–∑ —Å YOLO + SAM —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏–µ–π (—Ç–æ—á–Ω—ã–µ –º–∞—Å–∫–∏)"""
    return await process_image(file, include_sam=True, include_clip=False, include_llm=False, visualize=True)

@app.post("/detect/semantic")
async def detect_damage_semantic(file: UploadFile = File(...)):
    """–ê–Ω–∞–ª–∏–∑ —Å YOLO + CLIP (—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ)"""
    return await process_image(file, include_sam=False, include_clip=True, include_llm=False, visualize=True)

@app.post("/detect/professional")
async def detect_damage_professional(file: UploadFile = File(...)):
    """–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å–æ –≤—Å–µ–º–∏ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç—è–º–∏"""
    return await process_image(file, include_sam=True, include_clip=True, include_llm=True, visualize=True)

@app.get("/results")
async def get_results(limit: int = Query(10, description="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞"""
    try:
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        
        cursor.execute('''
            SELECT id, filename, timestamp, result, confidence, processing_time, model_version, modalities
            FROM detections
            ORDER BY timestamp DESC
            LIMIT ?
        ''', (limit,))
        
        rows = cursor.fetchall()
        conn.close()
        
        results = []
        for row in rows:
            result_data = {
                "id": row[0],
                "filename": row[1],
                "timestamp": row[2],
                "result": json.loads(row[3]) if row[3] else {},
                "confidence": row[4],
                "processing_time": row[5],
                "model_version": row[6],
                "modalities": json.loads(row[7]) if row[7] else []
            }
            results.append(result_data)
        
        return {"results": results, "total": len(results)}
        
    except Exception as e:
        logger.error(f"Error retrieving results: {e}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {str(e)}")

@app.get("/stats")
async def get_stats():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Å–∏—Å—Ç–µ–º—ã"""
    try:
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        
        cursor.execute('SELECT COUNT(*) FROM detections')
        total_detections = cursor.fetchone()[0]
        
        cursor.execute('SELECT AVG(confidence) FROM detections WHERE confidence > 0')
        avg_confidence = cursor.fetchone()[0] or 0.0
        
        cursor.execute('SELECT AVG(processing_time) FROM detections WHERE processing_time > 0')
        avg_processing_time = cursor.fetchone()[0] or 0.0
        
        cursor.execute('SELECT model_version, COUNT(*) FROM detections GROUP BY model_version')
        model_usage = dict(cursor.fetchall())
        
        cursor.execute('SELECT modalities, COUNT(*) FROM detections GROUP BY modalities')
        modality_usage = dict(cursor.fetchall())
        
        conn.close()
        
        return {
            "total_detections": total_detections,
            "average_confidence": round(avg_confidence, 3),
            "average_processing_time": round(avg_processing_time, 3),
            "model_usage": model_usage,
            "modality_usage": modality_usage,
            "system_info": {
                "version": "4.0.0-multi-modal",
                "mode": "multi_modal_sota",
                "modalities": ["yolo", "sam", "clip", "llm"],
                "torch_version": torch.__version__ if torch else "not_available",
                "cuda_available": torch.cuda.is_available() if torch else False,
                "pipeline_available": pipeline is not None
            }
        }
        
    except Exception as e:
        logger.error(f"Error retrieving stats: {e}")
        return {"error": str(e)}

@app.get("/summary/{result_id}")
async def get_analysis_summary(result_id: int):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ summary –∞–Ω–∞–ª–∏–∑–∞"""
    try:
        conn = sqlite3.connect(DB_PATH)
        cursor = conn.cursor()
        
        cursor.execute('SELECT result FROM detections WHERE id = ?', (result_id,))
        row = cursor.fetchone()
        conn.close()
        
        if not row:
            raise HTTPException(status_code=404, detail="–†–µ–∑—É–ª—å—Ç–∞—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        result_data = json.loads(row[0]) if row[0] else {}
        analysis_results = result_data.get("analysis_results", {})
        
        if not analysis_results:
            return {"summary": "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ summary"}
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º summary
        summary = pipeline.get_analysis_summary(analysis_results) if pipeline else "Pipeline –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"
        
        return {
            "result_id": result_id,
            "summary": summary,
            "analysis_results": analysis_results
        }
        
    except Exception as e:
        logger.error(f"Error generating summary: {e}")
        raise HTTPException(status_code=500, detail=f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ summary: {str(e)}")

if __name__ == "__main__":
    uvicorn.run(
        "src.api.main:app",
        host="0.0.0.0",
        port=8000,
        workers=1,
        log_level="info"
    )